"""
Enhanced Career Counseling Agent for RAG System
Uses semantic similarity instead of keyword matching
"""

from typing import List, Dict, Any, Optional
from loguru import logger
import time
import numpy as np

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

from rag_system.shared.config import RagSystemConfig
from rag_system.offline.document_store import DocumentStore
from rag_system.offline.embeddings import EmbeddingGenerator
from rag_system.offline.vector_store import VectorStore


class CareerCounselingAgent:
    """
    Enhanced Career Counseling Agent using semantic similarity
    instead of keyword-based domain detection
    """

    def __init__(self, config: RagSystemConfig):
        """
        Initialize Enhanced Career Counseling Agent
        
        Args:
            config: RagSystemConfig instance
        """
        self.config = config
        self.llm = None
        self.document_store = None
        self.embedding_generator = None
        self.vector_store = None
        
        # Thresholds for semantic filtering
        self.relevance_threshold = 0.6  # Minimum similarity score for relevance
        self.high_confidence_threshold = 0.8  # High confidence threshold
        
        self._initialize_components()
        self._setup_prompt_templates()
        self._initialize_domain_references()

    def _initialize_components(self) -> None:
        """Initialize RAG components"""
        try:
            # Initialize LLM
            self.llm = ChatOpenAI(
                model=self.config.openai_model_name,
                temperature=self.config.temperature,
                openai_api_key=self.config.openai_api_key
            )
            
            # Initialize RAG components
            self.document_store = DocumentStore(self.config)
            self.embedding_generator = EmbeddingGenerator(self.config)
            self.vector_store = VectorStore(self.config)
            
            logger.success("‚úÖ Enhanced career counseling agent initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize enhanced career agent: {str(e)}")
            raise

    def _setup_prompt_templates(self) -> None:
        """Setup custom prompt templates"""
        self.system_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ t∆∞ v·∫•n h∆∞·ªõng nghi·ªáp cho h·ªçc sinh, sinh vi√™n ho·∫∑c ng∆∞·ªùi ƒëang t√¨m ki·∫øm ƒë·ªãnh h∆∞·ªõng ngh·ªÅ nghi·ªáp.

Nhi·ªám v·ª• c·ªßa b·∫°n:
1. ƒê∆∞a ra l·ªùi khuy√™n chuy√™n nghi·ªáp v·ªÅ h∆∞·ªõng nghi·ªáp d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p
2. Tr·∫£ l·ªùi m·ªôt c√°ch r√µ r√†ng, chuy√™n nghi·ªáp v√† mang t√≠nh ƒë·ªãnh h∆∞·ªõng
3. N·∫øu ng·ªØ c·∫£nh kh√¥ng ƒë·ªß th√¥ng tin li√™n quan, h√£y ƒë∆∞a ra l·ªùi khuy√™n chung nh∆∞ng v·∫´n h·ªØu √≠ch
4. Lu√¥n gi·ªØ th√°i ƒë·ªô t√≠ch c·ª±c v√† h·ªó tr·ª£ ng∆∞·ªùi d√πng"""

        self.answer_template = """D∆∞·ªõi ƒë√¢y l√† ng·ªØ c·∫£nh t√†i li·ªáu li√™n quan (ƒë·ªô li√™n quan: {confidence_level}):

{context}

C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng: {question}

D·ª±a tr√™n ng·ªØ c·∫£nh tr√™n, h√£y tr·∫£ l·ªùi m·ªôt c√°ch r√µ r√†ng, chuy√™n nghi·ªáp v√† mang t√≠nh ƒë·ªãnh h∆∞·ªõng:"""

        self.insufficient_context_template = """C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng: {question}

T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ li√™n quan ƒë·∫øn c√¢u h·ªèi n√†y trong c∆° s·ªü d·ªØ li·ªáu hi·ªán t·∫°i. 
Tuy nhi√™n, v·ªõi t∆∞ c√°ch l√† tr·ª£ l√Ω t∆∞ v·∫•n ngh·ªÅ nghi·ªáp, t√¥i c√≥ th·ªÉ ƒë∆∞a ra m·ªôt s·ªë l·ªùi khuy√™n chung v√† h∆∞·ªõng d·∫´n b·∫°n t√¨m hi·ªÉu th√™m:"""

        self.off_topic_check_template = """H√£y ƒë√°nh gi√° xem c√¢u h·ªèi sau c√≥ li√™n quan ƒë·∫øn ngh·ªÅ nghi·ªáp, h·ªçc t·∫≠p, ho·∫∑c ƒë·ªãnh h∆∞·ªõng t∆∞∆°ng lai kh√¥ng:

C√¢u h·ªèi: "{question}"

Tr·∫£ l·ªùi ch·ªâ "C√ì" ho·∫∑c "KH√îNG" v√† gi·∫£i th√≠ch ng·∫Øn g·ªçn."""

    def _initialize_domain_references(self) -> None:
        """Initialize domain reference questions for semantic comparison"""
        self.career_reference_questions = [
            "N√™n h·ªçc ng√†nh g√¨ ƒë·ªÉ c√≥ t∆∞∆°ng lai t·ªët?",
            "L√†m th·∫ø n√†o ƒë·ªÉ ch·ªçn ngh·ªÅ ph√π h·ª£p?",
            "Ng√†nh c√¥ng ngh·ªá th√¥ng tin c√≥ tri·ªÉn v·ªçng kh√¥ng?",
            "M·ª©c l∆∞∆°ng c·ªßa ngh·ªÅ marketing nh∆∞ th·∫ø n√†o?",
            "T√¥i n√™n h·ªçc ƒë·∫°i h·ªçc hay h·ªçc ngh·ªÅ?",
            "K·ªπ nƒÉng n√†o c·∫ßn thi·∫øt ƒë·ªÉ xin vi·ªác?",
            "L√†m sao ƒë·ªÉ vi·∫øt CV ·∫•n t∆∞·ª£ng?",
            "C√°ch chu·∫©n b·ªã cho bu·ªïi ph·ªèng v·∫•n xin vi·ªác?",
            "Ngh·ªÅ n√†o ph√π h·ª£p v·ªõi ng∆∞·ªùi h∆∞·ªõng n·ªôi?",
            "H·ªçc b·∫±ng c·∫•p 2 c√≥ th·ªÉ l√†m ngh·ªÅ g√¨?",
        ]
        
        # Pre-compute embeddings for reference questions
        try:
            self.reference_embeddings = []
            for question in self.career_reference_questions:
                embedding = self.embedding_generator.generate_single(question)
                self.reference_embeddings.append(embedding)
            logger.info(f"‚úÖ Computed {len(self.reference_embeddings)} reference embeddings")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to compute reference embeddings: {str(e)}")
            self.reference_embeddings = []

    def is_greeting(self, question: str) -> bool:
        """Check if the question is a greeting"""
        greetings = [
            "xin ch√†o", "ch√†o", "hello", "hi", "hey",
            "ch√†o b·∫°n", "b·∫°n kh·ªèe kh√¥ng", "ch√∫c bu·ªïi s√°ng",
            "ch√∫c bu·ªïi chi·ªÅu", "ch√∫c bu·ªïi t·ªëi", "ch√†o bu·ªïi"
        ]
        
        question_lower = question.lower().strip()
        return any(greeting in question_lower for greeting in greetings)

    def compute_semantic_relevance(self, question: str) -> float:
        """
        Compute semantic relevance to career domain using reference questions
        
        Args:
            question: User's question
            
        Returns:
            float: Relevance score (0-1)
        """
        if not self.reference_embeddings:
            logger.warning("‚ö†Ô∏è No reference embeddings available, using fallback")
            return 0.5  # Neutral score if no references
        
        try:
            # Generate embedding for user question
            question_embedding = self.embedding_generator.generate_single(question)
            
            # Compute similarity with reference questions
            similarities = []
            for ref_embedding in self.reference_embeddings:
                # Cosine similarity
                similarity = np.dot(question_embedding, ref_embedding) / (
                    np.linalg.norm(question_embedding) * np.linalg.norm(ref_embedding)
                )
                similarities.append(similarity)
            
            # Return maximum similarity
            max_similarity = max(similarities)
            logger.info(f"üéØ Semantic relevance score: {max_similarity:.3f}")
            return max_similarity
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Semantic relevance computation failed: {str(e)}")
            return 0.5

    def retrieve_context_with_confidence(self, question: str, max_results: int = 5) -> tuple[List[Dict[str, Any]], float]:
        """
        Retrieve relevant context and compute confidence score
        
        Returns:
            tuple: (documents, max_confidence_score)
        """
        try:
            # Generate embedding for question
            question_embedding = self.embedding_generator.generate_single(question)
            
            # Search for similar documents
            similar_docs = self.vector_store.search_similar(
                query_embedding=question_embedding,
                k=max_results
            )
            
            # Compute confidence from similarity scores
            if similar_docs:
                scores = [doc.get('similarity_score', 0) for doc in similar_docs]
                max_confidence = max(scores) if scores else 0
            else:
                max_confidence = 0
            
            logger.info(f"üîç Retrieved {len(similar_docs)} docs, max confidence: {max_confidence:.3f}")
            return similar_docs, max_confidence
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Context retrieval failed: {str(e)}")
            return [], 0.0

    def format_context(self, documents: List[Dict[str, Any]], confidence: float) -> str:
        """Format retrieved documents into context string"""
        if not documents:
            return "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            content = doc.get("content", "")
            score = doc.get("similarity_score", 0)
            
            # Limit content length
            if len(content) > 500:
                content = content[:500] + "..."
            
            source = doc.get("metadata", {}).get("title", f"T√†i li·ªáu {i}")
            context_parts.append(f"[{source}] (ƒë·ªô li√™n quan: {score:.2f})\n{content}")
        
        return "\n\n".join(context_parts)

    def get_confidence_level_text(self, confidence: float) -> str:
        """Convert confidence score to descriptive text"""
        if confidence >= self.high_confidence_threshold:
            return "cao"
        elif confidence >= self.relevance_threshold:
            return "trung b√¨nh"
        else:
            return "th·∫•p"

    def generate_response(self, question: str, context: str, confidence: float) -> str:
        """Generate response using LLM with context and confidence"""
        try:
            confidence_level = self.get_confidence_level_text(confidence)
            
            if confidence >= self.relevance_threshold:
                # Use context-based template
                formatted_prompt = self.answer_template.format(
                    context=context,
                    question=question,
                    confidence_level=confidence_level
                )
            else:
                # Use insufficient context template
                formatted_prompt = self.insufficient_context_template.format(
                    question=question
                )
            
            messages = [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=formatted_prompt)
            ]
            
            # Generate response
            start_time = time.time()
            response = self.llm.invoke(messages)
            elapsed = time.time() - start_time
            
            logger.success(f"‚úÖ Generated response in {elapsed:.2f}s")
            return response.content
            
        except Exception as e:
            logger.error(f"‚ùå Response generation failed: {str(e)}")
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau."

    def check_topic_relevance_with_llm(self, question: str) -> bool:
        """
        Use LLM to check if question is career-related (fallback method)
        """
        try:
            formatted_prompt = self.off_topic_check_template.format(question=question)
            
            messages = [
                SystemMessage(content="B·∫°n l√† chuy√™n gia ph√¢n lo·∫°i c√¢u h·ªèi."),
                HumanMessage(content=formatted_prompt)
            ]
            
            response = self.llm.invoke(messages)
            response_text = response.content.strip().upper()
            
            is_relevant = "C√ì" in response_text
            logger.info(f"ü§ñ LLM topic check: {is_relevant}")
            return is_relevant
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LLM topic check failed: {str(e)}")
            return True  # Default to allowing the question

    def handle_greeting(self, question: str) -> str:
        """Handle greeting messages"""
        greetings_responses = [
            "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω t∆∞ v·∫•n h∆∞·ªõng nghi·ªáp. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi c√°c c√¢u h·ªèi v·ªÅ ngh·ªÅ nghi·ªáp, h·ªçc t·∫≠p v√† ƒë·ªãnh h∆∞·ªõng t∆∞∆°ng lai. B·∫°n c√≥ c√¢u h·ªèi g√¨ kh√¥ng?",
            "Ch√†o b·∫°n! R·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n v·ªÅ v·∫•n ƒë·ªÅ h∆∞·ªõng nghi·ªáp. B·∫°n ƒëang quan t√¢m ƒë·∫øn lƒ©nh v·ª±c n√†o?",
            "Hello! T√¥i chuy√™n t∆∞ v·∫•n v·ªÅ ngh·ªÅ nghi·ªáp v√† ƒë·ªãnh h∆∞·ªõng h·ªçc t·∫≠p. B·∫°n c·∫ßn t∆∞ v·∫•n v·ªÅ v·∫•n ƒë·ªÅ g√¨?"
        ]
        
        if "bu·ªïi s√°ng" in question.lower():
            return "Ch√†o bu·ªïi s√°ng! " + greetings_responses[0]
        elif "bu·ªïi chi·ªÅu" in question.lower():
            return "Ch√†o bu·ªïi chi·ªÅu! " + greetings_responses[1]
        elif "bu·ªïi t·ªëi" in question.lower():
            return "Ch√†o bu·ªïi t·ªëi! " + greetings_responses[2]
        else:
            return greetings_responses[0]

    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        Main method to answer career counseling questions using semantic approach
        
        Args:
            question: User's question
            
        Returns:
            Dict with response and metadata
        """
        start_time = time.time()
        
        # Trim and clean question
        question = question.strip()
        if not question:
            return {
                "response": "B·∫°n ch∆∞a ƒë·∫∑t c√¢u h·ªèi. H√£y h·ªèi t√¥i v·ªÅ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn ngh·ªÅ nghi·ªáp nh√©!",
                "type": "empty_question",
                "processing_time": 0
            }
        
        logger.info(f"ü§ñ Processing question: {question[:100]}...")
        
        # Handle greetings
        if self.is_greeting(question):
            response = self.handle_greeting(question)
            return {
                "response": response,
                "type": "greeting",
                "processing_time": time.time() - start_time
            }
        
        # Always retrieve context first
        relevant_docs, retrieval_confidence = self.retrieve_context_with_confidence(question)
        
        # Compute semantic relevance to career domain
        semantic_relevance = self.compute_semantic_relevance(question)
        
        # Combine retrieval confidence and semantic relevance
        combined_confidence = max(retrieval_confidence, semantic_relevance)
        
        # Decision logic based on confidence scores
        if combined_confidence < 0.3:
            # Very low confidence - likely off-topic
            # Use LLM as final check
            if not self.check_topic_relevance_with_llm(question):
                return {
                    "response": "T√¥i chuy√™n t∆∞ v·∫•n v·ªÅ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn ngh·ªÅ nghi·ªáp, h·ªçc t·∫≠p v√† ƒë·ªãnh h∆∞·ªõng t∆∞∆°ng lai. B·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi kh√°c li√™n quan ƒë·∫øn nh·ªØng ch·ªß ƒë·ªÅ n√†y kh√¥ng?",
                    "type": "off_topic",
                    "semantic_relevance": semantic_relevance,
                    "retrieval_confidence": retrieval_confidence,
                    "processing_time": time.time() - start_time
                }
        
        # Format context and generate response
        context = self.format_context(relevant_docs, retrieval_confidence)
        response = self.generate_response(question, context, combined_confidence)
        
        processing_time = time.time() - start_time
        
        # Determine response type based on confidence
        if combined_confidence >= self.high_confidence_threshold:
            response_type = "high_confidence_advice"
        elif combined_confidence >= self.relevance_threshold:
            response_type = "medium_confidence_advice"
        else:
            response_type = "general_advice"
        
        result = {
            "response": response,
            "type": response_type,
            "context_docs": len(relevant_docs),
            "semantic_relevance": semantic_relevance,
            "retrieval_confidence": retrieval_confidence,
            "combined_confidence": combined_confidence,
            "processing_time": processing_time
        }
        
        logger.success(f"‚úÖ Question answered in {processing_time:.2f}s")
        return result

    def update_domain_references(self, new_questions: List[str]) -> None:
        """
        Update domain reference questions for better semantic matching
        
        Args:
            new_questions: List of new reference questions
        """
        try:
            self.career_reference_questions.extend(new_questions)
            
            # Recompute all embeddings
            self.reference_embeddings = []
            for question in self.career_reference_questions:
                embedding = self.embedding_generator.generate_single(question)
                self.reference_embeddings.append(embedding)
            
            logger.success(f"‚úÖ Updated domain references: {len(self.career_reference_questions)} questions")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to update domain references: {str(e)}")

    def get_agent_info(self) -> Dict[str, Any]:
        """Get information about the enhanced agent"""
        return {
            "name": "Enhanced Career Counseling Agent",
            "domain": "career_counseling",
            "approach": "semantic_similarity",
            "capabilities": [
                "T∆∞ v·∫•n h∆∞·ªõng nghi·ªáp v·ªõi ƒë·ªô tin c·∫≠y cao",
                "Ph√¢n t√≠ch ng·ªØ nghƒ©a ƒë·ªÉ x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ",
                "ƒê·ªãnh h∆∞·ªõng h·ªçc t·∫≠p d·ª±a tr√™n ng·ªØ c·∫£nh",
                "ƒê√°nh gi√° ƒë·ªô li√™n quan t·ª± ƒë·ªông"
            ],
            "languages": ["Vietnamese"],
            "model": self.config.openai_model_name,
            "embedding_model": self.config.embedding_model_name,
            "relevance_threshold": self.relevance_threshold,
            "high_confidence_threshold": self.high_confidence_threshold,
            "reference_questions": len(self.career_reference_questions)
        }